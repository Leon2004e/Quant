# -*- coding: utf-8 -*-
"""
4_Regime_Builder/Trend_Achse/8.Regime_Report_Excel.py

Input:
  <ROOT>/1_Data_Center/Data/Regime/Performance/Trend/per_regime/<variant_id>/<TF>/<SYMBOL>.csv
  (generated by 7.Performance_Join.py)

Output:
  <ROOT>/1_Data_Center/Data/Regime/Performance/Trend/reports_excel/
      index.xlsx                      (mit Ranking/Clustering)
      per_regime_summary_ranked.csv    (Feature+Scores+Rank+Cluster)
      <variant_id>/<TF>/<SYMBOL>.xlsx  (Detail-Excel pro Regime)

Excel pro Regime:
  - Meta
  - State_Overview (by_state)
  - Hourly_Table (by_state_hour)
  - Hourly_Heatmap_mean, Hourly_Heatmap_n
  - Session_Table (by_state_session)
  - Session_Heatmap_mean, Session_Heatmap_n

Index.xlsx:
  - Index (Liste aller Files)
  - Ranked (sortiert nach composite_score)
  - Cluster_Summary

Ranking:
  - robust percentile ranks (0..1) statt willkürlicher Multiplikatoren
  - perf_score / safety_score / composite_score (0..1)

Clustering:
  - optional sklearn (KMeans + StandardScaler)
  - fallback: cluster_id=1, wenn sklearn fehlt oder zu wenig Daten

Install:
  pip install openpyxl
Optional für clustering:
  pip install scikit-learn
"""

from __future__ import annotations

import argparse
import os
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd

from openpyxl import Workbook
from openpyxl.styles import Font, Alignment
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.utils import get_column_letter
from openpyxl.formatting.rule import ColorScaleRule
from openpyxl.worksheet.worksheet import Worksheet

# optional clustering
try:
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    SKLEARN_OK = True
except Exception:
    SKLEARN_OK = False


# =========================
# ROOT / PATHS
# =========================

def find_project_root(start: Path) -> Path:
    cur = start.resolve()
    for p in [cur] + list(cur.parents):
        if (p / "1_Data_Center").exists():
            return p
    return start.resolve().parents[1]


ROOT = find_project_root(Path(__file__))

PERF_DIR = ROOT / "1_Data_Center" / "Data" / "Regime" / "Performance" / "Trend"
PER_REGIME_DIR = PERF_DIR / "per_regime"

REPORT_DIR = PERF_DIR / "reports_excel"
INDEX_XLSX = REPORT_DIR / "index.xlsx"
RANKED_CSV = REPORT_DIR / "per_regime_summary_ranked.csv"


# =========================
# Helpers
# =========================

def utc_now_str() -> str:
    return datetime.now(timezone.utc).isoformat(timespec="seconds")


def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def safe_read_csv(path: Path) -> pd.DataFrame:
    if not path.exists():
        return pd.DataFrame()
    df = pd.read_csv(path)
    return df if not df.empty else pd.DataFrame()


def split_sections(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    if df.empty or "section" not in df.columns:
        return {}
    out: Dict[str, pd.DataFrame] = {}
    for sec, g in df.groupby("section"):
        out[str(sec)] = g.drop(columns=["section"]).reset_index(drop=True)
    return out


def state_sort_key(v: int) -> int:
    # bear(-1), neutral(0), bull(1)
    return {-1: 0, 0: 1, 1: 2}.get(int(v), 99)


def to_numeric_safe(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def autofit_worksheet(ws: Worksheet, max_width: int = 48) -> None:
    for col_cells in ws.columns:
        col_letter = get_column_letter(col_cells[0].column)
        best = 8
        for cell in col_cells:
            if cell.value is None:
                continue
            try:
                ln = len(str(cell.value))
            except Exception:
                ln = 0
            if ln > best:
                best = ln
        ws.column_dimensions[col_letter].width = min(max_width, best + 2)


def set_header_style(ws: Worksheet, header_row: int = 1) -> None:
    for cell in ws[header_row]:
        cell.font = Font(bold=True)
        cell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)
    ws.freeze_panes = ws["A2"]


def write_df(ws: Worksheet, df: pd.DataFrame, start_row: int = 1, start_col: int = 1, header: bool = True) -> Tuple[int, int]:
    rows = dataframe_to_rows(df, index=False, header=header)
    r = start_row
    for row in rows:
        c = start_col
        for v in row:
            ws.cell(row=r, column=c, value=v)
            c += 1
        r += 1

    nrows = r - start_row
    ncols = df.shape[1]
    if header and not df.empty:
        set_header_style(ws, header_row=start_row)
    return nrows, ncols


def format_numbers(ws: Worksheet, df: pd.DataFrame, start_row: int, start_col: int) -> None:
    colnames = list(df.columns)
    for j, name in enumerate(colnames, start=start_col):
        fmt = None
        if name in ("mean_ret", "sum_ret"):
            fmt = "0.000000"
        elif name in ("vol_ret",):
            fmt = "0.000000"
        elif name in ("sharpe",):
            fmt = "0.000"
        elif name in ("hitrate",):
            fmt = "0.000"
        elif name in ("n",):
            fmt = "0"
        elif "hour" in name:
            fmt = "0"
        elif "core" in name:
            fmt = "0"

        if fmt is None:
            continue

        for i in range(start_row + 1, start_row + 1 + len(df)):
            ws.cell(row=i, column=j).number_format = fmt


def apply_heatmap_colors(ws: Worksheet, top_row: int, left_col: int, nrows: int, ncols: int) -> None:
    if nrows <= 0 or ncols <= 0:
        return
    start = f"{get_column_letter(left_col)}{top_row}"
    end = f"{get_column_letter(left_col + ncols - 1)}{top_row + nrows - 1}"
    rng = f"{start}:{end}"
    rule = ColorScaleRule(
        start_type="min", start_color="F8696B",     # red
        mid_type="num", mid_value=0, mid_color="FFEB84",  # yellow
        end_type="max", end_color="63BE7B",         # green
    )
    ws.conditional_formatting.add(rng, rule)


def pivot_matrix(df: pd.DataFrame, idx: str, col: str, val: str) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame()
    if idx not in df.columns or col not in df.columns or val not in df.columns:
        return pd.DataFrame()
    pvt = df.pivot_table(index=idx, columns=col, values=val, aggfunc="mean")
    pvt = pvt.sort_index()
    try:
        cols_sorted = sorted(list(pvt.columns), key=lambda x: state_sort_key(int(x)))
        pvt = pvt[cols_sorted]
    except Exception:
        pass
    pvt.columns = [f"state_{int(c)}" for c in pvt.columns]
    pvt = pvt.reset_index()
    return pvt


def parse_key_from_path(p: Path) -> Tuple[str, str, str]:
    symbol = p.stem
    tf = p.parent.name
    variant = p.parent.parent.name
    return variant, tf, symbol


def discover_per_regime_files() -> List[Path]:
    if not PER_REGIME_DIR.exists():
        return []
    return sorted(PER_REGIME_DIR.glob("*/*/*.csv"))


# =========================
# Feature extraction (for ranking)
# =========================

def _best_worst_disp_min_n(df: pd.DataFrame, state: int, metric: str, n_col: str = "n", min_n: int = 200) -> Tuple[float, float, float, float]:
    """
    Returns:
      best_mean, worst_mean, disp_mean, min_n_used
    Uses rows for given state and requires n>=min_n if n exists.
    """
    if df.empty:
        return (np.nan, np.nan, np.nan, np.nan)
    if "trend_state" not in df.columns or metric not in df.columns:
        return (np.nan, np.nan, np.nan, np.nan)

    d = df.copy()
    d = to_numeric_safe(d, ["trend_state", metric, n_col])
    d = d[d["trend_state"] == int(state)].copy()
    d = d.dropna(subset=[metric]).copy()
    if d.empty:
        return (np.nan, np.nan, np.nan, np.nan)

    if n_col in d.columns:
        d = d.dropna(subset=[n_col]).copy()
        d = d[d[n_col] >= int(min_n)].copy()
        if d.empty:
            return (np.nan, np.nan, np.nan, np.nan)
        mn = float(d[n_col].min())
    else:
        mn = np.nan

    best = float(d[metric].max())
    worst = float(d[metric].min())
    disp = float(d[metric].max() - d[metric].min())
    return (best, worst, disp, mn)


def extract_features_from_per_regime(per_regime_csv: Path) -> Dict[str, object]:
    df = safe_read_csv(per_regime_csv)
    if df.empty:
        raise RuntimeError(f"empty per_regime file: {per_regime_csv}")

    secs = split_sections(df)
    by_state = secs.get("by_state", pd.DataFrame())
    by_hour = secs.get("by_state_hour", pd.DataFrame())
    by_sess = secs.get("by_state_session", pd.DataFrame())

    # numeric normalization
    for d in (by_state, by_hour, by_sess):
        if not d.empty:
            to_numeric_safe(d, ["trend_state", "n", "mean_ret", "vol_ret", "sharpe", "hitrate", "sum_ret", "hour_utc", "session_core"])

    variant, tf, symbol = parse_key_from_path(per_regime_csv)

    out: Dict[str, object] = {
        "status": "ok",
        "variant_id": variant,
        "tf": tf,
        "symbol": symbol,
        "per_regime_csv": str(per_regime_csv),
    }

    # by_state summary per state
    for st, prefix in [(-1, "state-1"), (0, "state0"), (1, "state1")]:
        if by_state.empty or "trend_state" not in by_state.columns:
            out[f"{prefix}_n"] = np.nan
            out[f"{prefix}_mean"] = np.nan
            out[f"{prefix}_vol"] = np.nan
            out[f"{prefix}_sharpe"] = np.nan
            out[f"{prefix}_hit"] = np.nan
            continue

        row = by_state[by_state["trend_state"] == int(st)]
        if row.empty:
            out[f"{prefix}_n"] = np.nan
            out[f"{prefix}_mean"] = np.nan
            out[f"{prefix}_vol"] = np.nan
            out[f"{prefix}_sharpe"] = np.nan
            out[f"{prefix}_hit"] = np.nan
        else:
            r0 = row.iloc[0]
            out[f"{prefix}_n"] = float(r0["n"]) if "n" in row.columns and pd.notna(r0.get("n", np.nan)) else np.nan
            out[f"{prefix}_mean"] = float(r0["mean_ret"]) if "mean_ret" in row.columns and pd.notna(r0.get("mean_ret", np.nan)) else np.nan
            out[f"{prefix}_vol"] = float(r0["vol_ret"]) if "vol_ret" in row.columns and pd.notna(r0.get("vol_ret", np.nan)) else np.nan
            out[f"{prefix}_sharpe"] = float(r0["sharpe"]) if "sharpe" in row.columns and pd.notna(r0.get("sharpe", np.nan)) else np.nan
            out[f"{prefix}_hit"] = float(r0["hitrate"]) if "hitrate" in row.columns and pd.notna(r0.get("hitrate", np.nan)) else np.nan

    # hourly best/worst/disp/min_n (per state on mean_ret)
    for st, prefix in [(-1, "hour_state-1"), (0, "hour_state0"), (1, "hour_state1")]:
        best, worst, disp, mn = _best_worst_disp_min_n(by_hour, state=st, metric="mean_ret", min_n=200)
        out[f"{prefix}_best_mean"] = best
        out[f"{prefix}_worst_mean"] = worst
        out[f"{prefix}_disp_mean"] = disp
        out[f"{prefix}_min_n"] = mn

    # session best/worst/disp/min_n (per state on mean_ret)
    for st, prefix in [(-1, "sess_state-1"), (0, "sess_state0"), (1, "sess_state1")]:
        best, worst, disp, mn = _best_worst_disp_min_n(by_sess, state=st, metric="mean_ret", min_n=200)
        out[f"{prefix}_best_mean"] = best
        out[f"{prefix}_worst_mean"] = worst
        out[f"{prefix}_disp_mean"] = disp
        out[f"{prefix}_min_n"] = mn

    return out


# =========================
# Ranking + clustering
# =========================

def percentile_rank(s: pd.Series) -> pd.Series:
    x = pd.to_numeric(s, errors="coerce")
    # rank(pct=True) returns NaN where x is NaN -> keep
    return x.rank(pct=True, method="average")


def compute_scores_and_clusters(df: pd.DataFrame) -> pd.DataFrame:
    """
    Creates perf_score, safety_score, composite_score (0..1),
    rank (1=best), cluster_id.
    """
    d = df.copy()

    # mandatory columns might be missing depending on data; create safe NaNs
    need_cols = [
        "state1_sharpe", "state1_mean", "state1_hit",
        "state-1_sharpe", "state-1_mean",
        "coverage_join",
        "hour_state1_disp_mean", "sess_state1_disp_mean",
        "hour_state1_worst_mean", "sess_state1_worst_mean",
        "hour_state1_min_n", "sess_state1_min_n",
    ]
    for c in need_cols:
        if c not in d.columns:
            d[c] = np.nan

    # perf components (prefer bull)
    d["p_state1_sh"] = percentile_rank(d["state1_sharpe"])
    d["p_state1_m"] = percentile_rank(d["state1_mean"])
    d["p_state1_hit"] = percentile_rank(d["state1_hit"])

    # optional bear contribution (small weight; adjust if you trade short/mean-revert in bear)
    d["p_state-1_sh"] = percentile_rank(d["state-1_sharpe"])
    d["p_state-1_m"] = percentile_rank(d["state-1_mean"])

    # safety (smaller dispersion is better => 1 - pr)
    d["p_cov"] = percentile_rank(d["coverage_join"])
    d["p_hour_disp1"] = 1.0 - percentile_rank(d["hour_state1_disp_mean"])
    d["p_sess_disp1"] = 1.0 - percentile_rank(d["sess_state1_disp_mean"])

    # worst mean: higher is better (less negative)
    d["p_hour_worst1"] = percentile_rank(d["hour_state1_worst_mean"])
    d["p_sess_worst1"] = percentile_rank(d["sess_state1_worst_mean"])

    # min_n: bigger is better
    # combine hour+session min_n
    d["p_min_n1"] = 0.5 * percentile_rank(d["hour_state1_min_n"].fillna(0)) + 0.5 * percentile_rank(d["sess_state1_min_n"].fillna(0))

    # weights
    w_safety = 0.35
    w_perf_bull = 0.70
    w_perf_bear = 0.10

    d["perf_score"] = (
        w_perf_bull * (0.50*d["p_state1_sh"] + 0.35*d["p_state1_m"] + 0.15*d["p_state1_hit"]) +
        w_perf_bear * (0.60*d["p_state-1_sh"] + 0.40*d["p_state-1_m"])
    )

    d["safety_score"] = (
        0.35*d["p_cov"] +
        0.20*d["p_min_n1"] +
        0.15*d["p_hour_disp1"] +
        0.15*d["p_sess_disp1"] +
        0.075*d["p_hour_worst1"] +
        0.075*d["p_sess_worst1"]
    )

    d["composite_score"] = (1.0 - w_safety)*d["perf_score"] + w_safety*d["safety_score"]

    # rank (1 best)
    d["rank"] = d["composite_score"].rank(ascending=False, method="first").astype(int)

    # clustering
    d["cluster_id"] = 1
    if SKLEARN_OK:
        # choose robust features; fill NaNs with column medians
        feat_cols = [
            "state1_sharpe", "state1_mean", "state1_hit",
            "hour_state1_disp_mean", "sess_state1_disp_mean",
            "hour_state1_worst_mean", "sess_state1_worst_mean",
            "coverage_join",
        ]
        X = d[feat_cols].copy()
        for c in feat_cols:
            X[c] = pd.to_numeric(X[c], errors="coerce")
            med = float(np.nanmedian(X[c].to_numpy(dtype=float))) if np.isfinite(X[c]).any() else 0.0
            X[c] = X[c].fillna(med)

        n = len(X)
        if n >= 6:
            k = min(5, max(2, int(round(np.sqrt(n)))))
            scaler = StandardScaler()
            Xs = scaler.fit_transform(X.to_numpy(dtype=float))
            km = KMeans(n_clusters=k, random_state=42, n_init=10)
            d["cluster_id"] = km.fit_predict(Xs) + 1  # start at 1

    return d


# =========================
# Build one Excel
# =========================

def build_excel(per_regime_csv: Path, out_xlsx: Path) -> Dict[str, object]:
    df = safe_read_csv(per_regime_csv)
    if df.empty:
        raise RuntimeError(f"empty per_regime file: {per_regime_csv}")

    secs = split_sections(df)
    by_state = secs.get("by_state", pd.DataFrame())
    by_hour = secs.get("by_state_hour", pd.DataFrame())
    by_sess = secs.get("by_state_session", pd.DataFrame())

    for d in (by_state, by_hour, by_sess):
        if not d.empty:
            to_numeric_safe(d, ["trend_state", "n", "mean_ret", "vol_ret", "sharpe", "hitrate", "sum_ret", "hour_utc", "session_core"])

    if not by_state.empty and "trend_state" in by_state.columns:
        by_state = by_state.sort_values("trend_state", key=lambda s: s.map(lambda x: state_sort_key(int(x)))).reset_index(drop=True)

    def keep_cols(df0: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
        cols2 = [c for c in cols if c in df0.columns]
        return df0[cols2].copy() if cols2 else df0.copy()

    by_state = keep_cols(by_state, ["trend_label", "trend_state", "n", "mean_ret", "vol_ret", "sharpe", "hitrate", "sum_ret"])
    by_hour  = keep_cols(by_hour,  ["trend_label", "trend_state", "hour_utc", "n", "mean_ret", "vol_ret", "sharpe", "hitrate", "sum_ret"])
    by_sess  = keep_cols(by_sess,  ["trend_label", "trend_state", "session_block", "session_core", "n", "mean_ret", "vol_ret", "sharpe", "hitrate", "sum_ret"])

    hm_hour_mean = pivot_matrix(by_hour, idx="hour_utc", col="trend_state", val="mean_ret")
    hm_hour_n    = pivot_matrix(by_hour, idx="hour_utc", col="trend_state", val="n")

    if not by_sess.empty:
        sess2 = by_sess.copy()
        sess2["session_id"] = sess2["session_block"].astype(str) + "|core=" + sess2["session_core"].astype(int).astype(str)
        hm_sess_mean = pivot_matrix(sess2, idx="session_id", col="trend_state", val="mean_ret")
        hm_sess_n    = pivot_matrix(sess2, idx="session_id", col="trend_state", val="n")
    else:
        hm_sess_mean = pd.DataFrame()
        hm_sess_n = pd.DataFrame()

    wb = Workbook()
    wb.remove(wb.active)

    # Meta
    ws_meta = wb.create_sheet("Meta")
    variant, tf, symbol = parse_key_from_path(per_regime_csv)
    meta_rows = [
        ["variant_id", variant],
        ["tf", tf],
        ["symbol", symbol],
        ["source_csv", str(per_regime_csv)],
        ["generated_utc", utc_now_str()],
    ]
    for r, (k, v) in enumerate(meta_rows, start=1):
        ws_meta.cell(row=r, column=1, value=k).font = Font(bold=True)
        ws_meta.cell(row=r, column=2, value=v)
    ws_meta.column_dimensions["A"].width = 18
    ws_meta.column_dimensions["B"].width = 90

    # State overview
    ws_state = wb.create_sheet("State_Overview")
    if not by_state.empty:
        write_df(ws_state, by_state, start_row=1, start_col=1, header=True)
        format_numbers(ws_state, by_state, start_row=1, start_col=1)
        autofit_worksheet(ws_state)

    # Hourly table
    ws_ht = wb.create_sheet("Hourly_Table")
    if not by_hour.empty:
        if "trend_state" in by_hour.columns and "hour_utc" in by_hour.columns:
            by_hour = by_hour.sort_values(["trend_state", "hour_utc"]).reset_index(drop=True)
        write_df(ws_ht, by_hour, start_row=1, start_col=1, header=True)
        format_numbers(ws_ht, by_hour, start_row=1, start_col=1)
        autofit_worksheet(ws_ht)

    # Hourly heatmap mean
    ws_hm1 = wb.create_sheet("Hourly_Heatmap_mean")
    if not hm_hour_mean.empty:
        write_df(ws_hm1, hm_hour_mean, start_row=1, start_col=1, header=True)
        format_numbers(ws_hm1, hm_hour_mean, start_row=1, start_col=1)
        autofit_worksheet(ws_hm1)
        apply_heatmap_colors(ws_hm1, top_row=2, left_col=2, nrows=len(hm_hour_mean), ncols=hm_hour_mean.shape[1] - 1)

    # Hourly heatmap n
    ws_hm2 = wb.create_sheet("Hourly_Heatmap_n")
    if not hm_hour_n.empty:
        write_df(ws_hm2, hm_hour_n, start_row=1, start_col=1, header=True)
        format_numbers(ws_hm2, hm_hour_n, start_row=1, start_col=1)
        autofit_worksheet(ws_hm2)
        apply_heatmap_colors(ws_hm2, top_row=2, left_col=2, nrows=len(hm_hour_n), ncols=hm_hour_n.shape[1] - 1)

    # Session table
    ws_st = wb.create_sheet("Session_Table")
    if not by_sess.empty:
        if "trend_state" in by_sess.columns and "session_block" in by_sess.columns and "session_core" in by_sess.columns:
            by_sess = by_sess.sort_values(["trend_state", "session_block", "session_core"]).reset_index(drop=True)
        write_df(ws_st, by_sess, start_row=1, start_col=1, header=True)
        format_numbers(ws_st, by_sess, start_row=1, start_col=1)
        autofit_worksheet(ws_st)

    # Session heatmap mean
    ws_shm1 = wb.create_sheet("Session_Heatmap_mean")
    if not hm_sess_mean.empty:
        write_df(ws_shm1, hm_sess_mean, start_row=1, start_col=1, header=True)
        format_numbers(ws_shm1, hm_sess_mean, start_row=1, start_col=1)
        autofit_worksheet(ws_shm1, max_width=70)
        apply_heatmap_colors(ws_shm1, top_row=2, left_col=2, nrows=len(hm_sess_mean), ncols=hm_sess_mean.shape[1] - 1)

    # Session heatmap n
    ws_shm2 = wb.create_sheet("Session_Heatmap_n")
    if not hm_sess_n.empty:
        write_df(ws_shm2, hm_sess_n, start_row=1, start_col=1, header=True)
        format_numbers(ws_shm2, hm_sess_n, start_row=1, start_col=1)
        autofit_worksheet(ws_shm2, max_width=70)
        apply_heatmap_colors(ws_shm2, top_row=2, left_col=2, nrows=len(hm_sess_n), ncols=hm_sess_n.shape[1] - 1)

    ensure_dir(out_xlsx.parent)
    wb.save(out_xlsx)

    return {"ok": True, "in": str(per_regime_csv), "out": str(out_xlsx)}


# =========================
# Build index.xlsx (Index + Ranked + Cluster_Summary)
# =========================

def build_index_excel(index_rows: List[Dict[str, object]], ranked_df: pd.DataFrame) -> None:
    ensure_dir(REPORT_DIR)
    wb = Workbook()
    ws = wb.active
    ws.title = "Index"

    idx_df = pd.DataFrame(index_rows)
    if idx_df.empty:
        idx_df = pd.DataFrame(columns=["variant_id", "tf", "symbol", "xlsx_path", "per_regime_csv"])

    write_df(ws, idx_df, start_row=1, start_col=1, header=True)
    autofit_worksheet(ws, max_width=120)

    # Ranked sheet
    ws2 = wb.create_sheet("Ranked")
    if ranked_df is None or ranked_df.empty:
        ranked_df = pd.DataFrame(columns=["variant_id", "tf", "symbol", "composite_score", "perf_score", "safety_score", "rank", "cluster_id"])
    ranked_show_cols = [
        "rank", "cluster_id", "composite_score", "perf_score", "safety_score",
        "variant_id", "tf", "symbol",
        "coverage_join",
        "state1_sharpe", "state1_mean", "state1_hit",
        "hour_state1_disp_mean", "hour_state1_worst_mean", "hour_state1_min_n",
        "sess_state1_disp_mean", "sess_state1_worst_mean", "sess_state1_min_n",
        "per_regime_csv"
    ]
    ranked_out = ranked_df.copy()
    ranked_out = ranked_out[[c for c in ranked_show_cols if c in ranked_out.columns]].sort_values("rank")

    write_df(ws2, ranked_out, start_row=1, start_col=1, header=True)
    autofit_worksheet(ws2, max_width=140)

    # Cluster summary
    ws3 = wb.create_sheet("Cluster_Summary")
    if "cluster_id" in ranked_df.columns and not ranked_df.empty:
        g = ranked_df.groupby("cluster_id").agg(
            n=("cluster_id", "size"),
            mean_comp=("composite_score", "mean"),
            mean_perf=("perf_score", "mean"),
            mean_safe=("safety_score", "mean"),
        ).reset_index().sort_values("mean_comp", ascending=False)
    else:
        g = pd.DataFrame(columns=["cluster_id", "n", "mean_comp", "mean_perf", "mean_safe"])

    write_df(ws3, g, start_row=1, start_col=1, header=True)
    autofit_worksheet(ws3, max_width=40)

    wb.save(INDEX_XLSX)


# =========================
# CLI
# =========================

def parse_args() -> argparse.Namespace:
    ap = argparse.ArgumentParser()
    ap.add_argument("--max-n", type=int, default=0, help="limit number of reports (0=all)")
    ap.add_argument("--no-detail-excels", action="store_true", help="skip generating per-regime detail xlsx files")
    return ap.parse_args()


def main() -> None:
    args = parse_args()

    files = discover_per_regime_files()
    if not files:
        raise RuntimeError(f"No per_regime CSV files found under: {PER_REGIME_DIR}")

    if int(args.max_n) > 0:
        files = files[:int(args.max_n)]

    index_rows: List[Dict[str, object]] = []
    feat_rows: List[Dict[str, object]] = []

    counts = {"ok": 0, "error": 0}

    for f in files:
        variant, tf, symbol = parse_key_from_path(f)
        out_xlsx = REPORT_DIR / variant / tf / f"{symbol}.xlsx"

        # 1) detail excel (optional)
        if not bool(args.no_detail_excels):
            try:
                build_excel(f, out_xlsx)
                ok_path = str(out_xlsx)
            except Exception as e:
                ok_path = f"ERROR: {e}"
                counts["error"] += 1
            else:
                counts["ok"] += 1
        else:
            ok_path = str(out_xlsx)  # path placeholder

        index_rows.append({
            "variant_id": variant,
            "tf": tf,
            "symbol": symbol,
            "xlsx_path": ok_path,
            "per_regime_csv": str(f),
        })

        # 2) features for ranking
        try:
            feat = extract_features_from_per_regime(f)
            feat_rows.append(feat)
        except Exception as e:
            feat_rows.append({
                "status": "error",
                "variant_id": variant,
                "tf": tf,
                "symbol": symbol,
                "per_regime_csv": str(f),
                "error": str(e),
            })

    feat_df = pd.DataFrame(feat_rows)

    # add join coverage if you also have per_regime_summary.csv elsewhere:
    # Here: try infer coverage from file "summary.csv" is not available; keep NaN if missing.
    # If you already have coverage_join in another table, merge it before scoring.

    # Ensure coverage_join exists (if missing, set 1.0 so safety doesn't break; replace later with real)
    if "coverage_join" not in feat_df.columns:
        feat_df["coverage_join"] = 1.0

    ranked_df = feat_df.copy()
    ok_mask = ranked_df["status"].astype(str).str.lower().eq("ok")
    ranked_ok = ranked_df[ok_mask].copy()
    ranked_bad = ranked_df[~ok_mask].copy()

    if not ranked_ok.empty:
        ranked_ok = compute_scores_and_clusters(ranked_ok)
        ranked_ok = ranked_ok.sort_values("rank")
    else:
        ranked_ok["perf_score"] = np.nan
        ranked_ok["safety_score"] = np.nan
        ranked_ok["composite_score"] = np.nan
        ranked_ok["rank"] = np.nan
        ranked_ok["cluster_id"] = np.nan

    ranked_all = pd.concat([ranked_ok, ranked_bad], ignore_index=True)

    ensure_dir(REPORT_DIR)
    ranked_all.to_csv(RANKED_CSV, index=False)

    build_index_excel(index_rows=index_rows, ranked_df=ranked_ok)

    print("[INFO] detail_excels:", counts)
    print("[DONE] ranked_csv ->", RANKED_CSV.resolve())
    print("[DONE] index.xlsx  ->", INDEX_XLSX.resolve())
    if not SKLEARN_OK:
        print("[WARN] sklearn not available: clustering disabled (cluster_id=1). Install: pip install scikit-learn")


if __name__ == "__main__":
    main()
